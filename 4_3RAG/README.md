Building a RAG System: My Journey and Learnings
What I Built
I developed a Retrieval-Augmented Generation (RAG) system capable of answering questions specifically about artificial intelligence. The core idea is simple: it takes a given document, intelligently breaks it down into smaller, manageable chunks, and then, when a question is posed, it identifies the most relevant pieces of information from those chunks. Finally, it leverages an AI model to generate a coherent answer based only on that retrieved context.

It turned out to be significantly more challenging than I initially anticipated, but the moment it finally clicked and started working was incredibly rewarding!

The Source Document
For this project, I opted to use an overview of artificial intelligence that I authored myself. This decision came after some initial struggles with a Wikipedia scraper. My document covers a range of topics, including:

The fundamental definition of AI

Key concepts like machine learning and deep learning

Natural Language Processing (NLP)

Computer Vision

Robotics

Real-world AI applications and ethical considerations

Future prospects and inherent limitations

Weighing in at approximately 3,400 characters, it proved to be an ideal size for testing – comprehensive enough without being overwhelming.



Five Deep Dive Questions About RAG Systems
To truly grasp the mechanics of what I was building, I consulted ChatGPT with some fundamental questions about RAG systems. Here's a breakdown of what I learned:

1. Embedding Dimensionality: Unpacking 768 Dimensions

Q: What's the significance of the 768-dimensional embeddings generated by the sentence transformer model?

A: The all-distilroberta-v1 model produces 768-dimensional vectors for each segment of text. Imagine each sentence being described by 768 distinct numerical values that collectively capture its meaning. While higher dimensions allow for a more nuanced understanding, they also increase computational overhead. It seems 768 strikes a good balance between semantic richness and processing efficiency.



2. FAISS: Under the Hood

Q: How does the FAISS IndexFlatL2 search function internally?

A: FAISS, in this configuration, performs a "brute force" similarity search. It meticulously calculates the L2 (Euclidean) distance – essentially the straight-line distance in that 768-dimensional space – between your query's embedding and the embedding of every single chunk in the database. While not the fastest method for massive datasets, for my relatively small document, it's perfect, guaranteeing that the most similar chunks are always found.



3. The Importance of Chunk Overlap

Q: What are the consequences of not overlapping text chunks?

A: Without adequate overlap, crucial information can be inadvertently split across chunk boundaries. For instance, if a core concept is explained over two sentences that end up in different chunks, the complete explanation might be missed during retrieval. My chosen 50-character overlap helps mitigate this, ensuring contextual continuity.



4. Prompt Design: Does it Really Matter?

Q: How critical is the structure of the prompt provided to the language model?

A: Absolutely critical! I experimented with several approaches, and the prompt's format significantly impacts the model's performance. A clear, structured format like "Context: ... Question: ... Answer:" guides the model effectively. When I simply dumped the context and question together, the model often became confused and produced less accurate results.



5. Navigating Chunk Size Trade-offs

Q: How do I determine if 500 characters is the optimal chunk size?

A: It's a balancing act. Smaller chunks (e.g., 250 characters) are highly precise, pinpointing very specific relevant text, but they might lack broader context. Conversely, larger chunks (e.g., 1000 characters) offer more context but risk including irrelevant information that can distract or confuse the model. For my AI document, 500 characters seemed to hit the sweet spot, though the ideal size likely varies with content type.



Chunk Size Experiments: What Actually Happened
My experiments with different chunk sizes and overlaps yielded interesting results, highlighting the practical implications of these parameters:

Small Chunks (250 chars, 25 overlap)

Pros: Achieved high precision, often retrieving the exact sentences needed.

Cons: Answers sometimes felt incomplete or fragmented.

Best for: Simple, factual questions like "What is AI?"


Large Chunks (1000 chars, 100 overlap)

Pros: Provided more comprehensive explanations with richer context.

Cons: Occasionally pulled in extraneous, irrelevant information.

Best for: Complex questions requiring more background detail.


High Overlap (500 chars, 150 overlap)

Pros: Effectively prevented important information from being split.

Cons: Significantly increased storage requirements and slowed down processing.

Best for: Documents where concepts are highly interconnected.


My Sweet Spot: 500 Chars, 50 Overlap

For my AI document, this combination proved to be the most effective. It was large enough to encompass complete thoughts while remaining focused. The 50-character overlap successfully handled most instances where sentences might have been awkwardly split.

Quality of the Generated Responses
Strengths:

The system consistently demonstrated an understanding of my questions and retrieved genuinely relevant information.

Answers remained firmly grounded in the source document, avoiding "hallucinations."

FLAN-T5-small produced surprisingly clear and readable responses.

The entire setup ran locally on my laptop, which was a major plus.


Areas for Improvement:

Answers were sometimes too concise, limited by the 150-token cap.

Occasionally, the model would focus on only one aspect of a multi-part question.

Given its size, the FLAN-T5-small model isn't as sophisticated as larger models like ChatGPT.

Initial model loading times were quite long.


Illustrative Examples:

When asked "What is machine learning?", the system provided a solid, accurate answer directly from the relevant section of the document. For a question like "How does AI impact society?", it states "job displacement, privacy concerns, and the potential for bias in AI systems." Maybe I am spoiled by GPT and Google? Was looking for something a little more but I should probably manage expectations.


Future Enhancements & What I'd Do Differently
Technical Enhancements:

Experiment with a larger language model: While FLAN-T5-small works, a more powerful model would undoubtedly yield richer answers.

Implement re-ranking: Adding a step to verify if the retrieved chunks truly answer the question before generation could improve accuracy.

Integrate keyword search: Combining semantic search with traditional keyword matching could offer a more robust retrieval mechanism.

Smarter chunk boundaries: Splitting text based on sentence or paragraph structure, rather than arbitrary character counts, could preserve more meaning.


User Experience Improvements:

Display source quotes: Allowing users to see which specific parts of the document were used for an answer would build trust and transparency.

Add confidence scores: Indicating how confident the system is in its answers could be very useful.

Support multiple documents: Expanding the system to handle more than one source document would increase its utility.

Better error messages: My current error handling is quite basic; more informative messages would be beneficial.


Architectural Considerations:

Caching: Storing pre-computed embeddings would eliminate the need to recompute them on every run, speeding things up.

Streaming responses: Displaying answers as they are being generated would improve perceived performance.

Question suggestions: Offering related question prompts could enhance user interaction.

Export functionality: Allowing users to save Q&A sessions would add practical value.


Biggest Challenges Faced
Package Installation Hell: Getting all the necessary machine learning libraries to cooperate took significantly longer than the actual coding.

Model Downloads: The initial run, which involved downloading about 2GB of models, made me think the system was broken.

Understanding Embeddings: The concept of vector similarity and how text translates into numerical representations was initially quite perplexing.

Debugging Retrieval: When answers were incorrect, it was often difficult to pinpoint whether the issue lay in the retrieval phase or the generation phase.


Key Learnings
This project offered invaluable insights into the inner workings of modern AI systems. Before this, I admittedly viewed AI as somewhat magical. Now, I see it as a sophisticated interplay of:

Transforming text into numerical representations (embeddings).

Efficiently finding similar numerical patterns (vector search).

Leveraging these patterns to generate coherent responses (language models).

The RAG approach, in particular, is incredibly clever. It allows you to utilize a smaller, faster language model while still providing it access to specific, up-to-date knowledge. This is far more practical than attempting to train your own massive, general-purpose model.

Perhaps the most significant takeaway: a solid 90% of machine learning engineering seems to involve wrestling with package dependencies and data preprocessing, with the actual "AI" part being a smaller, albeit crucial, piece of the puzzle!

Conclusion
Building this RAG system was a tougher climb than anticipated, but the sense of accomplishment is immense. It genuinely works, providing surprisingly good answers to AI-related questions. While the code certainly has room for refinement and improvement, it successfully demonstrates all the core concepts of a RAG architecture.

It's clear why RAG has gained such popularity – it's like equipping AI systems with the ability to "look things up" in real-time, rather than solely relying on their pre-trained knowledge. Truly fascinating stuff!

Now, if you'll excuse me, I need to reclaim some hard drive space from all those downloaded model files... 😅